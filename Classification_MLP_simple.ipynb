{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slucey-cs-cmu-edu/RVSS26/blob/main/Classification_MLP_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAcFtWGpu3__"
      },
      "source": [
        "# Image classification with a multi-layer perceptron (MLP)\n",
        "\n",
        "This notebook is intentionally **minimal**. The goal is to make the *flow* obvious:\n",
        "\n",
        "1. Load data\n",
        "2. Define model (architecture)\n",
        "3. Define loss + optimiser\n",
        "4. Train with backprop (`loss.backward()`)\n",
        "5. Evaluate with accuracy\n",
        "\n",
        "You should be able to point to each of those steps in the code."
      ],
      "id": "OAcFtWGpu3__"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWFGbOzdu4AA"
      },
      "source": [
        "## 1. Imports + device"
      ],
      "id": "BWFGbOzdu4AA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhZ8FXAvu4AA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "# Simple reproducibility (optional)\n",
        "torch.manual_seed(0)"
      ],
      "id": "zhZ8FXAvu4AA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qih2ww9qu4AA"
      },
      "source": [
        "## 2. Data (MNIST)\n",
        "\n",
        "We split the original 60k training set into **train (50k)** and **val (10k)**. Test is the standard **10k** test set.\n",
        "\n",
        "We keep `num_workers=0` for Colab stability."
      ],
      "id": "qih2ww9qu4AA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKY6RNiou4AB"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean/std\n",
        "])\n",
        "\n",
        "full_train = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "test_set   = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_set, val_set = random_split(full_train, [50_000, 10_000], generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"train:\", len(train_set), \"val:\", len(val_set), \"test:\", len(test_set))"
      ],
      "id": "QKY6RNiou4AB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POQwFup7u4AB"
      },
      "source": [
        "### Visualise a small batch (optional)"
      ],
      "id": "POQwFup7u4AB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRZO8T-tu4AB"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "\n",
        "# Un-normalise roughly for display\n",
        "x_disp = x * 0.3081 + 0.1307\n",
        "grid = torchvision.utils.make_grid(x_disp[:32], nrow=8)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(grid.permute(1,2,0).numpy())\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"labels:\", y[:32].numpy())"
      ],
      "id": "hRZO8T-tu4AB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DB1xTEdu4AB"
      },
      "source": [
        "## 3. Model (architecture)\n",
        "\n",
        "Architecture is set here. This MLP is:\n",
        "\n",
        "`784 → hidden → hidden → 10`\n",
        "\n",
        "Hidden layers use ReLU. Output is **logits** (no softmax here)."
      ],
      "id": "6DB1xTEdu4AB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnz5SSQJu4AB"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden=256):\n",
        "        super().__init__()\n",
        "        self.fc0 = nn.Linear(28*28, hidden)\n",
        "        self.fc1 = nn.Linear(hidden, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)      # flatten: (B, 1, 28, 28) -> (B, 784)\n",
        "        x = F.relu(self.fc0(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "net = MLP(hidden=256).to(device)\n",
        "print(net)"
      ],
      "id": "mnz5SSQJu4AB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ2qr1r6u4AB"
      },
      "source": [
        "## 4. Loss and optimiser\n",
        "\n",
        "- Loss: `CrossEntropyLoss` (takes logits + labels)\n",
        "- Optimiser: AdamW (good modern default)\n",
        "\n",
        "Backprop is triggered by `loss.backward()` in the next section."
      ],
      "id": "AJ2qr1r6u4AB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYIScmlLu4AB"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-2)"
      ],
      "id": "BYIScmlLu4AB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-84dEsnUu4AB"
      },
      "source": [
        "## 5. Training loop\n",
        "\n",
        "This is the main flow:\n",
        "\n",
        "- forward pass\n",
        "- compute loss\n",
        "- `loss.backward()` (backprop)\n",
        "- `optimizer.step()` (update weights)\n",
        "\n",
        "We report validation accuracy each epoch."
      ],
      "id": "-84dEsnUu4AB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiqK1g5ru4AB"
      },
      "outputs": [],
      "source": [
        "def accuracy_from_logits(logits, y):\n",
        "    pred = logits.argmax(dim=1)\n",
        "    return (pred == y).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    net.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = net(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        bs = x.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy_from_logits(logits, y) * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    n = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for x, y in tqdm(train_loader, desc=f\"epoch {epoch}/{epochs}\", leave=False):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        logits = net(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        # backprop\n",
        "        loss.backward()\n",
        "\n",
        "        # update\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        running_loss += loss.item() * bs\n",
        "        n += bs\n",
        "\n",
        "    train_loss = running_loss / n\n",
        "    val_loss, val_acc = evaluate(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.2f}% | {time.time()-t0:.1f}s\")\n",
        "\n",
        "print(\"Finished training\")"
      ],
      "id": "yiqK1g5ru4AB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58IIvdDau4AB"
      },
      "source": [
        "## 6. Test set performance"
      ],
      "id": "58IIvdDau4AB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR6XDwwdu4AB"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = evaluate(test_loader)\n",
        "print(f\"TEST | loss={test_loss:.4f} | acc={test_acc*100:.2f}%\")"
      ],
      "id": "WR6XDwwdu4AB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KWecgXeu4AB"
      },
      "source": [
        "## 7. Softmax (optional)\n",
        "\n",
        "Training does **not** need softmax here. If you want probabilities, apply softmax to logits at inference time."
      ],
      "id": "6KWecgXeu4AB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_B_7mnIu4AC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_proba(x):\n",
        "    net.eval()\n",
        "    logits = net(x.to(device))\n",
        "    return torch.softmax(logits, dim=1).cpu()\n",
        "\n",
        "x, y = next(iter(test_loader))\n",
        "probs = predict_proba(x[:8])\n",
        "pred = probs.argmax(dim=1)\n",
        "\n",
        "print(\"pred:\", pred.numpy())\n",
        "print(\"true:\", y[:8].numpy())\n",
        "print(\"probs[0]:\", probs[0].numpy())"
      ],
      "id": "u_B_7mnIu4AC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtW6DEBmu4AC"
      },
      "source": [
        "## 8. Experiments\n",
        "\n",
        "Try these (one at a time):\n",
        "\n",
        "1. Change `hidden=256` to 128, 64, 32. When does accuracy start to drop?\n",
        "2. Change optimiser to SGD:\n",
        "   - `optim.SGD(..., lr=0.01, momentum=0.9)`\n",
        "3. Change activation to `tanh` by replacing `F.relu` with `torch.tanh`.\n",
        "4. Change epochs to 1, 3, 10 and compare."
      ],
      "id": "VtW6DEBmu4AC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PaJtPh1u4AC"
      },
      "source": [
        "## 9. Pixel permutations (key conceptual experiment)\n",
        "\n",
        "Fully connected networks **do not use spatial structure**.\n",
        "They treat the input as a vector, not as an image.\n",
        "\n",
        "Does **any fixed permutation of the pixels** give essentially the *same performance*?\n",
        "\n",
        "- ensure the permutation is applied consistently\n",
        "- the same permutation is used for train, validation, and test\n",
        "\n",
        "This is very different from tokenized networks like CNNs and Transformers, which *do* rely on\n",
        "local spatial structure."
      ],
      "id": "4PaJtPh1u4AC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbx2NJgVu4AC"
      },
      "source": [
        "### Step 1: Create a fixed random pixel permutation\n",
        "\n",
        "We create a single random permutation of the 784 input dimensions and\n",
        "keep it fixed for the entire experiment."
      ],
      "id": "Dbx2NJgVu4AC"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCPiHPsQ0_4z"
      },
      "id": "nCPiHPsQ0_4z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRobsz7ru4AC"
      },
      "outputs": [],
      "source": [
        "# Create a fixed random permutation of pixel indices\n",
        "perm = torch.randperm(28 * 28)\n",
        "\n",
        "def permute_pixels(x):\n",
        "    # x: (B, 1, 28, 28)\n",
        "    x = x.view(x.size(0), -1)      # (B, 784)\n",
        "    x = x[:, perm]                 # apply permutation\n",
        "    x = x.view(-1, 1, 28, 28)      # reshape back to image form\n",
        "    return x"
      ],
      "id": "GRobsz7ru4AC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfiEyGY1u4AC"
      },
      "source": [
        "### Step 2: Visualise original vs permuted images\n",
        "\n",
        "The permuted images no longer look like digits to *us*,\n",
        "but to an MLP they are just vectors."
      ],
      "id": "PfiEyGY1u4AC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdH7qtGDu4AC"
      },
      "outputs": [],
      "source": [
        "# Visualise original vs permuted images\n",
        "x, y = next(iter(train_loader))\n",
        "\n",
        "x_perm = permute_pixels(x)\n",
        "\n",
        "def show_side_by_side(x1, x2, labels, n=8):\n",
        "    import torchvision\n",
        "    x1_disp = x1 * 0.3081 + 0.1307\n",
        "    x2_disp = x2 * 0.3081 + 0.1307\n",
        "\n",
        "    grid1 = torchvision.utils.make_grid(x1_disp[:n], nrow=n)\n",
        "    grid2 = torchvision.utils.make_grid(x2_disp[:n], nrow=n)\n",
        "\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title(\"Original\")\n",
        "    plt.imshow(grid1.permute(1,2,0).numpy())\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title(\"Permuted pixels\")\n",
        "    plt.imshow(grid2.permute(1,2,0).numpy())\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "    print(\"labels:\", labels[:n].numpy())\n",
        "\n",
        "show_side_by_side(x, x_perm, y)"
      ],
      "id": "KdH7qtGDu4AC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMjdaDZuu4AC"
      },
      "source": [
        "### Step 3: Challenge\n",
        "\n",
        "Modify the training code so that **all inputs are permuted** before being\n",
        "passed to the network.\n",
        "\n",
        "Hints:\n",
        "- Apply `permute_pixels(x)` inside the training loop\n",
        "- Apply the same permutation in `evaluate(...)`\n",
        "- Do *not* change the model architecture\n",
        "\n",
        "Questions to think about:\n",
        "1. Does training accuracy change?\n",
        "2. Does validation/test accuracy change?\n",
        "3. Why does this work for MLPs but fail badly for CNNs?\n",
        "4. What does this say about inductive bias?"
      ],
      "id": "DMjdaDZuu4AC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-VUWVU3u4AC"
      },
      "source": [
        "**Takeaway**\n",
        "\n",
        "For fully connected networks, pixel order is arbitrary.\n",
        "All spatial meaning comes from the *data representation*, not the model.\n",
        "\n",
        "This experiment is one of the cleanest ways to see the difference between\n",
        "*representation* and *architecture*."
      ],
      "id": "R-VUWVU3u4AC"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}